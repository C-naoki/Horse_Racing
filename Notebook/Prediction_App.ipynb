{"cells":[{"cell_type":"markdown","metadata":{"id":"yXiRqh6jWx-l"},"source":["# 1: 事前準備"]},{"cell_type":"markdown","metadata":{"id":"DT3gxpVxWL_s"},"source":["## 1.1: インポート"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"06_zvzOQWMVj"},"outputs":[],"source":["from tqdm import tqdm\n","import sys\n","import time\n","import numpy as np\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","import re\n","import datetime\n","import lightgbm as lgb\n","from sklearn.metrics import roc_auc_score\n","from sklearn.preprocessing import LabelEncoder\n","import matplotlib.pyplot as plt\n","from itertools import combinations\n","from itertools import permutations\n","from urllib.request import urlopen"]},{"cell_type":"markdown","metadata":{"id":"BG4J6jhnWVX0"},"source":["## 1.2: pickleファイルの読み込み"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"wtWx2qe-WVuk"},"outputs":[],"source":["# 全体のデータを取得する。\n","race_results=pd.read_pickle('pickle/overall/race_results.pickle')\n","horse_results=pd.read_pickle('pickle/overall/horse_results.pickle')\n","ped_results=pd.read_pickle('pickle/overall/ped_results.pickle')\n","return_tables=pd.read_pickle('pickle/overall/return_tables.pickle')\n","horse_id_list = race_results['horse_id'].unique()\n","# 2021年のデータを取得する。\n","race_results_2021 = pd.read_pickle('pickle/2021/race_results.pickle')\n","horse_results_2021=pd.read_pickle('pickle/2021/horse_results.pickle')\n","return_tables_2021=pd.read_pickle('pickle/2021/return_tables.pickle')\n","ped_results_2021=pd.read_pickle('pickle/2021/ped_results.pickle')\n","horse_id_list_2021 = race_results_2021['horse_id'].unique()\n","# 2020年のデータを取得する。\n","race_results_2020 = pd.read_pickle('pickle/2020/race_results.pickle')\n","horse_results_2020=pd.read_pickle('pickle/2020/horse_results.pickle')\n","return_tables_2020=pd.read_pickle('pickle/2020/return_tables.pickle')\n","ped_results_2020=pd.read_pickle('pickle/2020/ped_results.pickle')\n","horse_id_list_2020 = race_results_2020['horse_id'].unique()\n","# 2019年のデータを取得する。\n","race_results_2019=pd.read_pickle('pickle/2019/race_results.pickle')\n","horse_results_2019=pd.read_pickle('pickle/2019/horse_results.pickle')\n","return_tables_2019=pd.read_pickle('pickle/2019/return_tables.pickle')\n","ped_results_2019=pd.read_pickle('pickle/2019/ped_results.pickle')\n","horse_id_list_2019 = race_results_2019['horse_id'].unique()\n","# 2018年のデータを取得する。\n","race_results_2018=pd.read_pickle('pickle/2018/race_results.pickle')\n","horse_results_2018=pd.read_pickle('pickle/2018/horse_results.pickle')\n","return_tables_2018=pd.read_pickle('pickle/2018/return_tables.pickle')\n","ped_results_2018=pd.read_pickle('pickle/2018/ped_results.pickle')\n","horse_id_list_2018 = race_results_2018['horse_id'].unique()\n","# 2017年のデータを取得する。\n","race_results_2017=pd.read_pickle('pickle/2017/race_results.pickle')\n","horse_results_2017=pd.read_pickle('pickle/2017/horse_results.pickle')\n","return_tables_2017=pd.read_pickle('pickle/2017/return_tables.pickle')\n","ped_results_2017=pd.read_pickle('pickle/2017/ped_results.pickle')\n","horse_id_list_2017 = race_results_2017['horse_id'].unique()"]},{"cell_type":"markdown","metadata":{"id":"4n-eSNFoWbGg"},"source":["## 1.3: 必要リスト"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"LcHZBtHwWe_m"},"outputs":[],"source":["#開催場所をidに変換するための辞書型\n","place_dict = {\n","    '札幌':'01',  '函館':'02',  '福島':'03',  '新潟':'04',  '東京':'05', \n","    '中山':'06',  '中京':'07',  '京都':'08',  '阪神':'09',  '小倉':'10'\n","}\n","\n","#レースタイプをレース結果データと整合させるための辞書型\n","race_type_dict = {\n","    '芝': '芝', 'ダ': 'ダート', '障': '障害'\n","}"]},{"cell_type":"markdown","metadata":{"id":"an_d15ZmWhb0"},"source":["# 2: クラス定義"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"CR7j_GT0WmQP"},"outputs":[],"source":["# 基底クラス\n","class DataProcessor:\n","\n","    \"\"\"    \n","    Attributes:\n","    ----------\n","    data : pd.DataFrame\n","        rawデータ\n","    data_p : pd.DataFrame\n","        preprocessing後のデータ\n","    data_h : pd.DataFrame\n","        merge_horse_results後のデータ\n","    data_pe : pd.DataFrame\n","        merge_peds後のデータ\n","    data_c : pd.DataFrame\n","        process_categorical後のデータ\n","    no_peds: Numpy.array\n","        merge_pedsを実行した時に、血統データが存在しなかった馬のhorse_id一覧\n","    \"\"\"\n","\n","\n","    def __init__(self):\n","        self.data = pd.DataFrame()\n","        self.data_p = pd.DataFrame()\n","        self.data_h = pd.DataFrame()\n","        self.data_pe = pd.DataFrame()\n","        self.data_c = pd.DataFrame()\n","\n","    # shutuba_tables_pに過去データを追記する。\n","    def merge_horse_results(self, hr, n_samples_list=[5, 9, 'all']):\n","\n","        \"\"\"\n","        馬の過去成績データから、\n","        n_samples_listで指定されたレース分の着順と賞金の平均を追加してdata_hに返す\n","        Parameters:\n","        ----------\n","        hr : HorseResults\n","            馬の過去成績データ\n","        n_samples_list : list, default [5, 9, 'all']\n","            過去何レース分追加するか\n","        \"\"\"\n","\n","        self.data_h = self.data_p.copy()\n","        for n_samples in n_samples_list:\n","            self.data_h = hr.merge_all(self.data_h, n_samples=n_samples)\n","        \n","        # 馬の出走間隔\n","        self.data_h['interval'] = (self.data_h['date'] - self.data_h['latest']).dt.days\n","        self.data_h.drop(['開催', 'latest'], axis=1, inplace=True)\n","\n","    # pedデータを追記する。\n","    def merge_ped_results(self, ped_results):\n","\n","        \"\"\"\n","        5世代分血統データを追加してdata_peに返す\n","        Parameters:\n","        ----------\n","        peds : Peds.peds_e\n","            Pedsクラスで加工された血統データ。\n","        \"\"\"\n","\n","        self.data_pe = self.data_h.merge(ped_results, left_on='horse_id', right_index=True, how='left')\n","\n","        self.no_peds = self.data_pe[self.data_pe['peds_0'].isnull()]['horse_id'].unique()\n","        if len(self.no_peds) > 0:\n","            print('please scrape ped_results at horse_id_list \"no_peds\"')\n","\n","    # カテゴリ変数の処理\n","    def process_categorical(self, le_horse, le_jockey, results_m):\n","\n","        \"\"\"\n","        カテゴリ変数を処理してdata_cに返す\n","        Parameters:\n","        ----------\n","        le_horse : sklearn.preprocessing.LabelEncoder\n","            horse_idを0始まりの整数に変換するLabelEncoderオブジェクト。\n","        le_jockey : sklearn.preprocessing.LabelEncoder\n","            jockey_idを0始まりの整数に変換するLabelEncoderオブジェクト。\n","        results_m : Results.data_pe\n","            ダミー変数化のとき、ResultsクラスとShutubaTableクラスで列を合わせるためのもの\n","        \"\"\"\n","\n","        df = self.data_pe.copy()\n","\n","        # ラベルエンコーディング: horse_id, jockey_idを0始まりの整数に変換\n","        # classes_: fit()によって各ラベルがどのラベルIDと対応づけられたのかを取得可能\n","        # mask関数: 引数リストの中身がTrueのところをNaNにし、Falseのところには実データを入れる\n","        # where関数: 引数リストの中身がTrueのところを実データにし、FalseのところにはNaNを入れる\n","        mask_horse = df['horse_id'].isin(le_horse.classes_)\n","        new_horse_id = df['horse_id'].mask(mask_horse).dropna().unique()\n","        le_horse.classes_ = np.concatenate([le_horse.classes_, new_horse_id])\n","        df['horse_id'] = le_horse.transform(df['horse_id'])\n","        mask_jockey = df['jockey_id'].isin(le_jockey.classes_)\n","        new_jockey_id = df['jockey_id'].mask(mask_jockey).dropna().unique()\n","        le_jockey.classes_ = np.concatenate([le_jockey.classes_, new_jockey_id])\n","        df['jockey_id'] = le_jockey.transform(df['jockey_id'])\n","\n","        # horse_id, jockey_idをpandasのcategory型に変換\n","        df['horse_id'] = df['horse_id'].astype('category')\n","        df['jockey_id'] = df['jockey_id'].astype('category')\n","\n","        # その他のカテゴリ変数をpandasのcategory型に変換してからダミー変数化\n","        # 列を一定にするため\n","        weathers = results_m['weather'].unique()\n","        race_types = results_m['race_type'].unique()\n","        ground_states = results_m['ground_state'].unique()\n","        sexes = results_m['性'].unique()\n","\n","        # pd.Categorical関数: get_dummies関数にかける前に列を指定できる。要するに、全て0の列を作成可能。\n","        df['weather'] = pd.Categorical(df['weather'], weathers)\n","        df['race_type'] = pd.Categorical(df['race_type'], race_types)\n","        df['ground_state'] = pd.Categorical(df['ground_state'], ground_states)\n","        df['性'] = pd.Categorical(df['性'], sexes)\n","        df = pd.get_dummies(df, columns=['weather', 'race_type', 'ground_state', '性'])\n","\n","        self.data_c = df\n","\n","# レースに関するクラス\n","class RaceResults(DataProcessor):\n","\n","    def __init__(self, race_results):\n","        super(RaceResults, self).__init__()\n","        self.data = race_results\n","\n","    @classmethod\n","    def read_pickle(cls, path_list):\n","        df = pd.read_pickle(path_list[0])\n","        for path in path_list[1:]:\n","            df = update_data(df, pd.read_pickle(path))\n","        return cls(df)\n","\n","    @staticmethod\n","    def scrape(race_id_list, pre_race_results=pd.DataFrame()):\n","\n","        \"\"\"\n","        レース結果データをスクレイピングする関数\n","        Parameters:\n","        ----------\n","        race_id_list : list\n","            レースIDのリスト\n","        Returns:\n","        ----------\n","        race_results_df : pandas.DataFrame\n","            全レース結果データをまとめてDataFrame型にしたもの\n","        \"\"\"\n","\n","        #race_idをkeyにしてDataFrame型を格納\n","        race_results = {}\n","        for race_id in tqdm(race_id_list):\n","            R = int(race_id) % 100\n","            day = (int(race_id) % 10000 - R)//100\n","            kai = (int(race_id) % 1000000  - R - 100 * day) // 10000\n","            place = (int(race_id) % 100000000  - R - 100 * day  - 10000 * kai) // 1000000\n","            # print(R+12*(day-1)+144*(kai-1)+1728*(place-1))\n","            # if R+12*(day-1)+144*(kai-1)+1728*(place-1) == 10000:\n","            #   break\n","            if len(pre_race_results) and int(race_id) <= int(pre_race_results.index[-1]):\n","                continue\n","            try:\n","                time.sleep(1)\n","                url = \"https://db.netkeiba.com/race/\" + race_id\n","                #メインとなるテーブルデータを取得\n","                df = pd.read_html(url)[0]\n","                html = requests.get(url)\n","                html.encoding = \"EUC-JP\"\n","                soup = BeautifulSoup(html.text, \"html.parser\")\n","                #天候、レースの種類、コースの長さ、馬場の状態、日付をスクレイピング\n","                texts = (\n","                    soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[0].text\n","                    + soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[1].text\n","                )\n","                info = re.findall(r'\\w+', texts)\n","                for text in info:\n","                    if text in [\"芝\", \"ダート\"]:\n","                        df[\"race_type\"] = [text] * len(df)\n","                    if \"障\" in text:\n","                        df[\"race_type\"] = [\"障害\"] * len(df)\n","                    if \"m\" in text:\n","                        df[\"course_len\"] = [int(re.findall(r\"\\d+\", text)[0])] * len(df)\n","                    if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n","                        df[\"ground_state\"] = [text] * len(df)\n","                    if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n","                        df[\"weather\"] = [text] * len(df)\n","                    if \"年\" in text:\n","                        df[\"date\"] = [text] * len(df)\n","                #馬ID、騎手IDをスクレイピング\n","                horse_id_list = []\n","                horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n","                    \"a\", attrs={\"href\": re.compile(\"^/horse\")}\n","                )\n","                for a in horse_a_list:\n","                    horse_id = re.findall(r\"\\d+\", a[\"href\"])\n","                    horse_id_list.append(horse_id[0])\n","                jockey_id_list = []\n","                jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n","                    \"a\", attrs={\"href\": re.compile(\"^/jockey\")}\n","                )\n","                for a in jockey_a_list:\n","                    jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n","                    jockey_id_list.append(jockey_id[0])\n","                df[\"horse_id\"] = horse_id_list\n","                df[\"jockey_id\"] = jockey_id_list\n","                #インデックスをrace_idにする\n","                df.index = [race_id] * len(df)\n","                race_results[race_id] = df\n","            #存在しないrace_idを飛ばす\n","            except IndexError:\n","                continue\n","            #wifiの接続が切れた時などでも途中までのデータを返せるようにする\n","            except Exception as e:\n","                print(e)\n","                break\n","            #Jupyterで停止ボタンを押した時の対処\n","            except:\n","                break\n","        #pd.DataFrame型にして一つのデータにまとめる\n","        race_results_df = pd.concat([race_results[key] for key in race_results])\n","        if len(pre_race_results.index):\n","            return pd.concat([pre_race_results, race_results_df])\n","        else:\n","            return race_results_df\n","\n","    # 前処理\n","    def preprocessing(self):\n","        df = self.data.copy()\n","        # 着順に数字以外のものが含まれているデータを取り除く\n","        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n","        df.dropna(subset=['着順'], inplace=True)\n","        df['着順'] = df['着順'].astype(int)\n","        df['rank'] = df['着順'].map(lambda x: 1 if x<4 else 0)\n","        # 性齢を性と年齢に分割\n","        df['性'] = df['性齢'].map(lambda x: str(x)[0])\n","        df['年齢'] = df['性齢'].map(lambda x: str(x)[1]).astype(int)\n","        # 馬体重を現体重と増減に分割\n","        # expand:Falseなら1列に分割後の配列が格納される\n","        df['体重'] = df['馬体重'].str.split(\"(\", expand=True)[0].astype(int)\n","        df['体重変化'] = df['馬体重'].str.split(\"(\", expand=True)[1].str[:-1].astype(int)\n","        df['単勝'] = df['単勝'].astype(float)\n","        df[\"course_len\"] = df[\"course_len\"].astype(float) // 100\n","        df['date'] = pd.to_datetime(df['date'], format='%Y年%m月%d日')\n","        # 開催地\n","        df['開催'] = df.index.map(lambda x:str(x)[4:6])\n","        # 出走数\n","        df['n_horses'] = df.index.map(df.index.value_counts())\n","        # 不要な列を削除する\n","        # inplace:dfを置き換える\n","        df.drop(['タイム', '着差', '調教師', '性齢', '馬体重', '馬名', '騎手', '人気', '着順'], axis='columns', inplace=True)\n","        self.data_p = df\n","    def to_rank(self):\n","        df = self.data.copy()\n","        df['rank'] = df['着順'].map(lambda x: x if x<4 else 4)\n","        df.drop(['着順'], axis='columns', inplace=True)\n","        self.data = df\n","    def process_categorical(self):\n","        self.le_horse = LabelEncoder().fit(self.data_pe['horse_id'])\n","        self.le_jockey = LabelEncoder().fit(self.data_pe['jockey_id'])\n","        super().process_categorical(self.le_horse, self.le_jockey, self.data_pe)\n","# 馬に関するクラス\n","class HorseResults:\n","\n","    def __init__(self, horse_results):\n","        self.horse_results = horse_results[['日付', '着順', '賞金', '着差', '通過', '開催', '距離']]\n","        self.preprocessing()\n","    @classmethod\n","    def read_pickle(cls, path_list):\n","        df = pd.read_pickle(path_list[0])\n","        for path in path_list[1:]:\n","            df = update_data(df, pd.read_pickle(path))\n","        return cls(df)\n","    @staticmethod\n","    def scrape(horse_id_list, pre_horse_results=pd.DataFrame()):\n","        \"\"\"\n","        馬の過去成績データをスクレイピングする関数\n","\n","        Parameters:\n","        ----------\n","        horse_id_list : list\n","            馬IDのリスト\n","\n","        Returns:\n","        ----------\n","        horse_results_df : pandas.DataFrame\n","            全馬の過去成績データをまとめてDataFrame型にしたもの\n","        \"\"\"\n","        #horse_idをkeyにしてDataFrame型を格納\n","        horse_results = {}\n","\n","        for horse_id in tqdm(horse_id_list):\n","            if horse_id in pre_horse_results.index.unique():\n","                continue\n","            try:\n","                url = 'https://db.netkeiba.com/horse/' + horse_id\n","                df = pd.read_html(url)[3]\n","                #受賞歴がある馬の場合、3番目に受賞歴テーブルが来るため、4番目のデータを取得する\n","                if df.columns[0]=='受賞歴':\n","                    df = pd.read_html(url)[4]\n","                df.index = [horse_id] * len(df)\n","                horse_results[horse_id] = df\n","                time.sleep(1)\n","            except IndexError:\n","                continue\n","            except Exception as e:\n","                print(e)\n","                break\n","            except:\n","                break\n","        #pd.DataFrame型にして一つのデータにまとめる        \n","        horse_results_df = pd.concat([horse_results[key] for key in horse_results])\n","        if len(pre_horse_results):\n","            return pd.concat([pre_horse_results, horse_results_df]) \n","        else:\n","            return horse_results_df\n","\n","    def preprocessing(self):\n","        df = self.horse_results.copy()\n","\n","        # 着順に数字以外のものが含まれているデータを取り除く\n","        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n","        df.dropna(subset=['着順'], inplace=True)\n","        df['着順'] = df['着順'].astype(int)\n","\n","        df['date'] = pd.to_datetime(df['日付'])\n","        df.drop(['日付'], axis='columns', inplace=True)\n","\n","        # 賞金=NaNを0で埋める\n","        df['賞金'].fillna(0, inplace=True)\n","\n","        # 1着の着差を0とする。(元データには2位との着差を負の値で格納されている)\n","        df['着差'] = df['着差'].map(lambda x: 0 if x<0 else x)\n","\n","        # レース展開データ\n","        # n=1: 最初のコーナーの位置, n=4: 最終コーナー位置\n","        def corner(x, n):\n","            if type(x) != str:\n","                return x\n","            elif n == 4:\n","                return int(re.findall(r'\\d+', x)[-1])\n","            elif n == 1:\n","                return int(re.findall(r'\\d+', x)[0])\n","        df['first_corner'] = df['通過'].map(lambda x: corner(x, 1))\n","        df['final_corner'] = df['通過'].map(lambda x: corner(x, 4))\n","\n","        df['final_to_rank'] = df['final_corner'] - df['着順']\n","        df['first_to_rank'] = df['first_corner'] - df['着順']\n","        df['first_to_final'] = df['first_corner'] - df['final_corner']\n","\n","        #開催場所\n","        df['開催'] = df['開催'].str.extract(r'(\\D+)')[0].map(place_dict).fillna('11')\n","\n","        #race_type\n","        df['race_type'] = df['距離'].str.extract(r'(\\D+)')[0].map(race_type_dict)\n","\n","        #距離は10のくらいで切り捨てる。\n","        df['course_len'] = df['距離'].str.extract(r'(\\d+)').astype(int) // 100\n","        df.drop(['距離'], axis=1, inplace=True)\n","\n","        #インデックス名を与える\n","        df.index.name = 'horse_id'\n","\n","        self.horse_results = df\n","        self.target_list = ['着順', '賞金', '着差', 'first_corner', 'final_corner', 'first_to_rank', 'first_to_final', 'final_to_rank']\n","    # 日付ごとの着順と賞金の平均値を算出する関数\n","    def average(self, horse_id_list, date, n_samples='all'):\n","        # query関数: pd.DataFrameのデータの内、条件を満たす物だけを抽出する。\n","        #            @は変数名を利用したいときに先頭につける。\n","        #            これで、horse_results内に無いデータにも対応可能。(但し、格納されているデータは欠損値)\n","        target_df = self.horse_results.query('index in @horse_id_list')\n","\n","        # 過去何回分の平均を取り出すか設定\n","        if n_samples == 'all':\n","            filtered_df = target_df[target_df['date'] < date]\n","        elif n_samples > 0:\n","            # ある日付より前の日付という条件の下、n_samples個のデータのみを抽出して平均値を出す。\n","            filtered_df = target_df[target_df['date'] < date].sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n","        else:\n","            raise Exception('n_sample must be \"all\" or plus number')\n","    \n","        self.average_dict = {}\n","            # add_suffix関数: 列名の最後に引数の文字列を追加する関数。\n","        self.average_dict['non_category'] = filtered_df.groupby(level=0)[self.target_list].mean().add_suffix('_{}R'.format(n_samples))\n","        for column in ['course_len', 'race_type', '開催']:\n","            self.average_dict[column] = filtered_df.groupby(['horse_id', column])[self.target_list].mean().add_suffix('_{}_{}R'.format(column, n_samples))\n","    \n","        # 馬の出走間隔追加のために、全レースの日付を変数latestに格納\n","        if n_samples == 5:\n","            self.latest = filtered_df.groupby('horse_id')['date'].max().rename('latest')\n","\n","    # dateをkeyにしてdfを結合する関数\n","    def merge(self, results, date, n_samples='all'):\n","        df = results[results['date']==date]\n","        horse_id_list = df['horse_id']\n","        self.average(horse_id_list, date, n_samples)\n","        merged_df = df.merge(self.average_dict['non_category'], left_on='horse_id', right_index=True, how='left')\n","        for column in ['course_len', 'race_type', '開催']:\n","            merged_df = merged_df.merge(self.average_dict[column],\n","                                        left_on=['horse_id', column],\n","                                        right_index=True,\n","                                        how='left')\n","        # 馬の出走間隔追加のために、全レースの日付を変数latestに格納\n","        if n_samples == 5:\n","            merged_df = merged_df.merge(self.latest, left_on='horse_id', right_index=True, how='left')\n","        return merged_df\n","\n","    # 上記の操作を全日付に対して行う関数\n","    def merge_all(self, results, n_samples='all'):\n","        date_list = results['date'].unique()\n","        merged_df = pd.concat([self.merge(results, date, n_samples) for date in tqdm(date_list)])\n","        return merged_df\n","# 血統データを処理するクラス\n","class Peds:\n","    def __init__(self, ped_results):\n","        self.ped_results = ped_results\n","        self.ped_results_e = pd.DataFrame() # after label encoding and transforming into category\n","\n","    @classmethod\n","    def read_pickle(cls, path_list):\n","        df = pd.read_pickle(path_list[0])\n","        for path in path_list[1:]:\n","            df = update_data(df, pd.read_pickle(path))\n","        return cls(df)\n","    \n","    @staticmethod\n","    def scrape(horse_id_list, pre_ped_results=pd.DataFrame()):\n","        \"\"\"\n","        血統データをスクレイピングする関数\n","\n","        Parameters:\n","        ----------\n","        horse_id_list : list\n","            馬IDのリスト\n","\n","        Returns:\n","        ----------\n","        peds_df : pandas.DataFrame\n","            全血統データをまとめてDataFrame型にしたもの\n","        \"\"\"\n","\n","        peds_dict = {}\n","        for horse_id in tqdm(horse_id_list):\n","            if horse_id in pre_ped_results.index.unique():\n","                continue\n","            try:\n","                url = \"https://db.netkeiba.com/horse/ped/\" + horse_id\n","                df = pd.read_html(url)[0]\n","\n","                #重複を削除して1列のSeries型データに直す\n","                generations = {}\n","                for i in reversed(range(5)):\n","                    generations[i] = df[i]\n","                df.drop([i], axis='columns', inplace=True)\n","                df = df.drop_duplicates()\n","                ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)\n","\n","                peds_dict[horse_id] = ped.reset_index(drop=True)\n","                time.sleep(1)\n","            except IndexError:\n","                continue\n","            except Exception as e:\n","                print(e)\n","                break\n","            except:\n","                break\n","\n","        #列名をpeds_0, ..., peds_61にする\n","        peds_df = pd.concat([peds_dict[key] for key in peds_dict], axis=1).T.add_prefix('peds_')\n","\n","        if len(pre_ped_results):\n","            return pd.concat([pre_ped_results, peds_df]) \n","        else:\n","            return peds_df\n","    \n","    def encode(self):\n","        df = self.ped_results.copy()\n","        for column in df.columns:\n","            df[column] = LabelEncoder().fit_transform(df[column].fillna('Na'))\n","        self.ped_results_e = df.astype('category')\n","\n","class Return:\n","    def __init__(self, return_tables):\n","        self.return_tables = return_tables\n","    @classmethod\n","    def read_pickle(cls, path_list):\n","        df = pd.read_pickle(path_list[0])\n","        for path in path_list[1:]:\n","            df = update_data(df, pd.read_pickle(path))\n","        return cls(df)\n","    @staticmethod\n","    def scrape(race_id_list, pre_race_tables=pd.DataFrame()):\n","        #race_idをkeyにしてDataFrame型を格納\n","        race_tables = {}\n","        for race_id in tqdm(race_id_list):\n","            if race_id in pre_race_tables.index.unique():\n","                continue\n","            try:\n","                time.sleep(1)\n","                url = \"https://db.netkeiba.com/race/\" + race_id\n","                f = urlopen(url)\n","                html = f.read()\n","                html = html.replace(b'<br />', b'br')\n","                dfs = pd.read_html(html)\n","                dfs[1].index = [race_id] * len(dfs[1])\n","                dfs[2].index = [race_id] * len(dfs[2])\n","                race_tables[race_id] = pd.concat([dfs[1], dfs[2]])\n","            except IndexError:\n","                continue\n","            #wifiの接続が切れた時などでも途中までのデータを返せるようにする\n","            except Exception as e:\n","                print(e)\n","                break\n","            #Jupyterで停止ボタンを押した時の対処\n","            except:\n","                break\n","        race_tables_df = pd.concat([race_tables[key] for key in race_tables])\n","        if len(pre_race_tables):\n","            return pd.concat([pre_race_tables, race_tables_df])\n","        else:\n","            return race_tables_df\n","    # 関数を引数のように扱うことが可能なclassの手法\n","    @property\n","    def fukusho(self):\n","        fukusho = self.return_tables[self.return_tables[0]=='複勝'][[1, 2]]\n","        wins = fukusho[1].str.split('br', expand=True)[[0, 1, 2]]\n","        wins.columns = ['win_0', 'win_1', 'win_2']\n","        returns = fukusho[2].str.split('br', expand=True)[[0, 1, 2]]\n","        returns.columns = ['return_0', 'return_1', 'return_2']\n","        # axis=1:DataFrameを横に接続する(default=0)\n","        df = pd.concat([wins, returns], axis=1)\n","        for column in df.columns:\n","            df[column] = df[column].str.replace(',', '')\n","        return df.fillna(0).astype(int)\n","    @property\n","    def tansho(self):\n","        tansho = self.return_tables[self.return_tables[0]=='単勝'][[1, 2]]\n","        tansho.columns = ['win', 'return']\n","        for column in tansho.columns:\n","            tansho[column] = pd.to_numeric(tansho[column], errors='coerce')\n","        return tansho\n","    @property\n","    def umaren(self):\n","        umaren = self.return_tables[self.return_tables[0]=='馬連'][[1,2]]\n","        wins = umaren[1].str.split('-', expand=True)[[0,1]].add_prefix('win_')\n","        return_ = umaren[2].rename('return')  \n","        df = pd.concat([wins, return_], axis=1)        \n","        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n","    @property\n","    def umatan(self):\n","        umatan = self.return_tables[self.return_tables[0]=='馬単'][[1,2]]\n","        wins = umatan[1].str.split('→', expand=True)[[0,1]].add_prefix('win_')\n","        return_ = umatan[2].rename('return')  \n","        df = pd.concat([wins, return_], axis=1)        \n","        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n","    @property\n","    def wide(self):\n","        wide = self.return_tables[self.return_tables[0]=='ワイド'][[1,2]]\n","        wins = wide[1].str.split('br', expand=True)[[0,1,2]]\n","        wins = wins.stack().str.split('-', expand=True).add_prefix('win_')\n","        return_ = wide[2].str.split('br', expand=True)[[0,1,2]]\n","        return_ = return_.stack().rename('return')\n","        df = pd.concat([wins, return_], axis=1)\n","        return df.apply(lambda x: pd.to_numeric(x.str.replace(',',''), errors='coerce'))\n","    @property\n","    def sanrentan(self):\n","        rentan = self.return_tables[self.return_tables[0]=='三連単'][[1,2]]\n","        wins = rentan[1].str.split('→', expand=True)[[0,1,2]].add_prefix('win_')\n","        return_ = rentan[2].rename('return')\n","        df = pd.concat([wins, return_], axis=1) \n","        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n","    @property\n","    def sanrenpuku(self):\n","        renpuku = self.return_tables[self.return_tables[0]=='三連複'][[1,2]]\n","        wins = renpuku[1].str.split('-', expand=True)[[0,1,2]].add_prefix('win_')\n","        return_ = renpuku[2].rename('return')\n","        df = pd.concat([wins, return_], axis=1) \n","        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n","# 自動でChromeを開いて検索してくれる。\n","# Chromeで実際に開いてからじゃないとJavaScriptのデータを取り出せない(らしい)\n","# googlecolabのやり方(本来はもっと楽にできる。)\n","\n","class ShutubaTable(DataProcessor):\n","    def __init__(self, shutuba_tables):\n","        super(ShutubaTable ,self).__init__()\n","        self.data = shutuba_tables\n","\n","    @classmethod\n","    def scrape(cls, race_id_list, date):\n","        data = pd.DataFrame()\n","        for race_id in tqdm(race_id_list):\n","            url = 'https://race.netkeiba.com/race/shutuba.html?race_id=' + race_id\n","            df = pd.read_html(url)[0]\n","            df = df.T.reset_index(level=0, drop=True).T\n","            html = requests.get(url)\n","            html.encoding = 'EUC-JP'\n","            soup = BeautifulSoup(html.text, 'html.parser')\n","            texts = soup.find('div', attrs={'class': 'RaceData01'}).text\n","            texts = re.findall(r'\\w+', texts)\n","            for text in texts:\n","                if 'm' in text:\n","                    df['course_len'] = [int(re.findall(r'\\d+', text)[0])] * len(df)\n","                if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n","                    df[\"weather\"] = [text] * len(df)\n","                if text in [\"良\", \"稍重\", \"重\"]:\n","                    df[\"ground_state\"] = [text] * len(df)\n","                if '不' in text:\n","                    df[\"ground_state\"] = ['不良'] * len(df)\n","                if '稍' in text:\n","                    df[\"ground_state\"] = ['稍重'] * len(df)\n","                if '芝' in text:\n","                    df['race_type'] = ['芝'] * len(df)\n","                if '障' in text:\n","                    df['race_type'] = ['障害'] * len(df)\n","                if 'ダ' in text:\n","                    df['race_type'] = ['ダート'] * len(df)\n","            df['date'] = [date] * len(df)\n","            # horse_id\n","            horse_id_list = list()\n","            horse_td_list = soup.find_all(\"td\", attrs={'class': 'HorseInfo'})\n","            for td in horse_td_list:\n","                horse_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n","                horse_id_list.append(horse_id)\n","            df['horse_id'] = horse_id_list\n","            # jockey_id\n","            jockey_id_list = list()\n","            jockey_td_list = soup.find_all(\"td\", attrs={'class': 'Jockey'})\n","            for td in jockey_td_list:\n","                jockey_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n","                jockey_id_list.append(jockey_id)\n","            df['jockey_id'] = jockey_id_list\n","            df.index = [race_id] * len(df)\n","            data = data.append(df)\n","            time.sleep(1)\n","        return cls(data)\n","    # 前処理\n","    def preprocessing(self):\n","        df = self.data.copy()\n","        # 性齢を性と年齢に分割\n","        df['性'] = df['性齢'].map(lambda x: str(x)[0])\n","        df['年齢'] = df['性齢'].map(lambda x: str(x)[1]).astype(int)\n","        # 馬体重を現体重と増減に分割\n","        # expand:Falseなら1列に分割後の配列が格納される\n","        df = df[df['馬体重(増減)'] != '--']\n","        df['体重'] = df['馬体重(増減)'].str.split(\"(\", expand=True)[0].astype(int)\n","        df['体重変化'] = df['馬体重(増減)'].str.split(\"(\", expand=True)[1].str[:-1].astype(int)\n","        df['体重変化'] = pd.to_numeric(df['体重変化'], errors='coerce')\n","        df['date'] = pd.to_datetime(df['date'])\n","        df['枠'] = df['枠'].astype(int)\n","        df['馬番'] = df['馬番'].astype(int)\n","        df['斤量'] = df['斤量'].astype(int)\n","        # 開催地\n","        df['開催'] = df.index.map(lambda x:str(x)[4:6])\n","        # 出走数\n","        df['n_horses'] = df.index.map(df.index.value_counts())\n","        df = df[['枠', '馬番', '斤量', 'course_len', 'weather', 'race_type', 'ground_state', 'date', 'horse_id', 'jockey_id', '性', '年齢', '体重', '体重変化', '開催', 'n_horses']]\n","        self.data_p = df.rename(columns={'枠': '枠番'})\n","class ModelEvaluator:\n","    def __init__(self, model, return_tables_path_list):\n","        self.model = model\n","        self.rt = Return.read_pickle(return_tables_path_list)\n","        self.fukusho = self.rt.fukusho\n","        self.tansho = self.rt.tansho\n","        self.umaren = self.rt.umaren\n","        self.umatan = self.rt.umatan\n","        self.wide = self.rt.wide\n","        self.sanrentan = self.rt.sanrentan\n","        self.sanrenpuku = self.rt.sanrenpuku\n","    def predict_proba(self, X, train=True, std=True, minmax=False):\n","        if train:\n","            proba = pd.Series(\n","                self.model.predict_proba(X.drop(['単勝'], axis=1))[:, 1], index=X.index\n","            )\n","        else:\n","            proba = pd.Series(\n","                self.model.predict_proba(X, axis=1)[:, 1], index=X.index\n","            )\n","        if std:\n","            #レース内で標準化して、相対評価する。「レース内偏差値」みたいなもの。\n","            standard_scaler = lambda x: (x - x.mean()) / x.std()\n","            proba = proba.groupby(level=0).transform(standard_scaler)\n","        if minmax:\n","            #データ全体を0~1にする\n","            proba = (proba - proba.min()) / (proba.max() - proba.min())\n","        return proba\n","\n","    def predict(self, X, threshold=0.5):\n","        y_pred = self.predict_proba(X)\n","        self.proba = y_pred\n","        return [0 if p<threshold else 1 for p in y_pred]\n","    def score(self, y_true, X):\n","        return roc_auc_score(y_true, self.predict_proba(X))\n","    def feature_importance(self, X, n_display=20):\n","        importances = pd.DataFrame({'features': X.columns,\n","                                    'importance': self.model.feature_importances_})\n","        return importances.sort_values(\"importance\", ascending=False)[:n_display]\n","    def pred_table(self, X, threshold=0.5, bet_only=True):\n","        pred_table = X.copy()[['馬番', '単勝']]\n","        pred_table['pred'] = self.predict(X, threshold)\n","        pred_table['score'] = self.proba\n","        if bet_only:\n","            return pred_table[pred_table['pred']==1][['馬番', '単勝', 'score']]\n","        else:\n","            return pred_table[['馬番', '単勝', 'score', 'pred']]\n","    def bet(self, race_id, kind, umaban, amount):\n","        if kind == 'fukusho':\n","            rt_1R = self.fukusho.loc[race_id]\n","            return_ = (rt_1R[['win_0', 'win_1', 'win_2']]==umaban).values * rt_1R[['return_0', 'return_1', 'return_2']].values * amount/100\n","            return_ = np.sum(return_)\n","        if kind == 'tansho':\n","            rt_1R = self.tansho.loc[race_id]\n","            return_ = (rt_1R['win']==umaban) * rt_1R['return'] * amount/100\n","        if kind == 'umaren':\n","            rt_1R = self.umaren.loc[race_id]\n","            return_ = (set(rt_1R[['win_0', 'win_1']]) == set(umaban)) * rt_1R['return']/100 * amount\n","        if kind == 'umatan':\n","            rt_1R = self.umatan.loc[race_id]\n","            return_ = (list(rt_1R[['win_0', 'win_1']]) == list(umaban)) * rt_1R['return']/100 * amount\n","        if kind == 'wide':\n","            rt_1R = self.wide.loc[race_id]\n","            return_ = (rt_1R[['win_0', 'win_1']].apply(lambda x: set(x)==set(umaban), axis=1)) * rt_1R['return']/100 * amount\n","            return_ = return_.sum()\n","        if kind == 'sanrentan':\n","            rt_1R = self.sanrentan.loc[race_id]\n","            return_ = (list(rt_1R[['win_0', 'win_1', 'win_2']]) == list(umaban)) * rt_1R['return']/100 * amount\n","        if kind == 'sanrenpuku':\n","            rt_1R = self.sanrenpuku.loc[race_id]\n","            return_ = (set(rt_1R[['win_0', 'win_1', 'win_2']]) == set(umaban)) * rt_1R['return']/100 * amount\n","        if not (return_ >= 0):\n","            return_ = amount\n","        return return_\n","    def fukusho_return(self, X, threshold=0.5):\n","        pred_table = self.pred_table(X, threshold)\n","        n_bets = len(pred_table)\n","        return_list = list()\n","        for race_id, preds in pred_table.groupby(level=0):\n","            return_list.append(np.sum([\n","                self.bet(race_id, 'fukusho', umaban, 1) for umaban in preds['馬番']\n","            ]))\n","        return_rate = np.sum(return_list) / n_bets\n","        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n","        n_hits = np.sum([x>0 for x in return_list])\n","        return n_bets, return_rate, n_hits, std\n","    def tansho_return(self, X, threshold=0.5):\n","        pred_table = self.pred_table(X, threshold)\n","        self.sample = pred_table\n","        n_bets = len(pred_table)\n","        return_list = list()\n","        for race_id, preds in pred_table.groupby(level=0):\n","            return_list.append(\n","                np.sum([self.bet(race_id, 'tansho', umaban, 1) for umaban in preds['馬番']])\n","            )\n","        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n","        n_hits = np.sum([x>0 for x in return_list])\n","        return_rate = np.sum(return_list) / n_bets\n","        return n_bets, return_rate, n_hits, std\n","    def tansho_return_proper(self, X, threshold=0.5):\n","        #モデルによって、「賭ける」と判断された馬たち\n","        pred_table = self.pred_table(X, threshold)\n","        n_bets = len(pred_table)\n","        return_list = list()\n","        for race_id, preds in pred_table.groupby(level=0):\n","            return_list.append(np.sum(preds.apply(lambda x: self.bet(race_id, 'tansho', x['馬番'], 1/x['単勝']), axis=1)))\n","        bet_money = (1 / pred_table['単勝']).sum()\n","        std = np.std(return_list) * np.sqrt(len(return_list)) / bet_money\n","        n_hits = np.sum([x>0 for x in return_list])\n","        return_rate = np.sum(return_list) / bet_money\n","        return n_bets, return_rate, n_hits, std\n","    def umaren_box(self, X, threshold=0.5, n_aite=5):\n","        pred_table = self.pred_table(X, threshold, bet_only = False)\n","        n_bets = 0\n","        \n","        return_list = []\n","        for race_id, preds in pred_table.groupby(level=0):\n","            return_ = 0\n","            preds_jiku = preds.query('pred == 1')\n","            if len(preds_jiku) == 1:\n","                continue\n","            elif len(preds_jiku) >= 2:\n","                for umaban in combinations(preds_jiku['馬番'], 2):\n","                    return_ += self.bet(race_id, 'umaren', umaban, 1)\n","                    n_bets += 1\n","                return_list.append(return_)\n","        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n","        n_hits = np.sum([x>0 for x in return_list])\n","        return_rate = np.sum(return_list) / n_bets\n","        return n_bets, return_rate, n_hits, std\n","    def umatan_box(self, X, threshold=0.5, n_aite=5):\n","        pred_table = self.pred_table(X, threshold, bet_only = False)\n","        n_bets = 0\n","        \n","        return_list = []\n","        for race_id, preds in pred_table.groupby(level=0):\n","            return_ = 0\n","            preds_jiku = preds.query('pred == 1')\n","            if len(preds_jiku) == 1:\n","                continue   \n","            elif len(preds_jiku) >= 2:\n","                for umaban in permutations(preds_jiku['馬番'], 2):\n","                    return_ += self.bet(race_id, 'umatan', umaban, 1)\n","                    n_bets += 1\n","            return_list.append(return_)\n","        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n","        n_hits = np.sum([x>0 for x in return_list])\n","        return_rate = np.sum(return_list) / n_bets\n","        return n_bets, return_rate, n_hits, std\n","    def wide_box(self, X, threshold=0.5, n_aite=5):\n","        pred_table = self.pred_table(X, threshold, bet_only = False)\n","        n_bets = 0\n","        return_list = []\n","        for race_id, preds in pred_table.groupby(level=0):\n","            return_ = 0\n","            preds_jiku = preds.query('pred == 1')\n","            if len(preds_jiku) == 1:\n","                continue\n","            elif len(preds_jiku) >= 2:\n","                for umaban in combinations(preds_jiku['馬番'], 2):\n","                    return_ += self.bet(race_id, 'wide', umaban, 1)\n","                    n_bets += 1\n","                    return_list.append(return_)\n","        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n","        n_hits = np.sum([x>0 for x in return_list])\n","        return_rate = np.sum(return_list) / n_bets\n","        return n_bets, return_rate, n_hits, std  \n","def sanrentan_box(self, X, threshold=0.5):\n","    pred_table = self.pred_table(X, threshold)\n","    n_bets = 0\n","    \n","    return_list = []\n","    for race_id, preds in pred_table.groupby(level=0):\n","        return_ = 0\n","        if len(preds)<3:\n","            continue\n","        else:\n","            for umaban in permutations(preds['馬番'], 3):\n","                return_ += self.bet(race_id, 'sanrentan', umaban, 1)\n","                n_bets += 1\n","            return_list.append(return_)\n","    std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n","    n_hits = np.sum([x>0 for x in return_list])\n","    return_rate = np.sum(return_list) / n_bets\n","    return n_bets, return_rate, n_hits, std\n","def sanrenpuku_box(self, X, threshold=0.5):\n","    pred_table = self.pred_table(X, threshold)\n","    n_bets = 0\n","    return_list = []\n","    for race_id, preds in pred_table.groupby(level=0):\n","        return_ = 0\n","        if len(preds)<3:\n","            continue\n","        else:\n","            for umaban in combinations(preds['馬番'], 3):\n","                return_ += self.bet(race_id, 'sanrenpuku', umaban, 1)\n","                n_bets += 1\n","            return_list.append(return_)\n","    std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n","    n_hits = np.sum([x>0 for x in return_list])\n","    return_rate = np.sum(return_list) / n_bets\n","    return n_bets, return_rate, n_hits, std\n","def umaren_nagashi(self, X, threshold=0.5, n_aite=5):\n","    pred_table = self.pred_table(X, threshold, bet_only = False)\n","    n_bets = 0\n","    return_list = []\n","    for race_id, preds in pred_table.groupby(level=0):\n","        return_ = 0\n","        preds_jiku = preds.query('pred == 1')\n","        if len(preds_jiku) == 1:\n","            preds_aite = preds.sort_values('score', ascending = False).iloc[1:(n_aite+1)]['馬番']\n","            return_ = preds_aite.map(\n","                lambda x: self.bet(\n","                    race_id, 'umaren', [preds_jiku['馬番'].values[0], x], 1\n","                )\n","            ).sum()\n","            n_bets += n_aite\n","            return_list.append(return_)\n","        elif len(preds_jiku) >= 2:\n","            for umaban in combinations(preds_jiku['馬番'], 2):\n","                return_ += self.bet(race_id, 'umaren', umaban, 1)\n","                n_bets += 1\n","            return_list.append(return_)\n","    std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n","    n_hits = np.sum([x>0 for x in return_list])\n","    return_rate = np.sum(return_list) / n_bets\n","    return n_bets, return_rate, n_hits, std\n","def umatan_nagashi(self, X, threshold=0.5, n_aite=5):\n","    pred_table = self.pred_table(X, threshold, bet_only = False)\n","    n_bets = 0\n","    \n","    return_list = []\n","    for race_id, preds in pred_table.groupby(level=0):\n","        return_ = 0\n","        preds_jiku = preds.query('pred == 1')\n","        if len(preds_jiku) == 1:\n","            preds_aite = preds.sort_values('score', ascending = False).iloc[1:(n_aite+1)]['馬番']\n","            return_ = preds_aite.map(\n","                lambda x: self.bet(\n","                    race_id, 'umatan', [preds_jiku['馬番'].values[0], x], 1\n","                )\n","            ).sum()\n","            n_bets += n_aite\n","        elif len(preds_jiku) >= 2:\n","            for umaban in permutations(preds_jiku['馬番'], 2):\n","                return_ += self.bet(race_id, 'umatan', umaban, 1)\n","                n_bets += 1\n","            return_list.append(return_)\n","    std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n","    n_hits = np.sum([x>0 for x in return_list])\n","    return_rate = np.sum(return_list) / n_bets\n","    return n_bets, return_rate, n_hits, std\n","def wide_nagashi(self, X, threshold=0.5, n_aite=5):\n","    pred_table = self.pred_table(X, threshold, bet_only = False)\n","    n_bets = 0\n","    return_list = []\n","    for race_id, preds in pred_table.groupby(level=0):\n","        return_ = 0\n","        preds_jiku = preds.query('pred == 1')\n","        if len(preds_jiku) == 1:\n","            preds_aite = preds.sort_values('score', ascending = False)\\\n","                .iloc[1:(n_aite+1)]['馬番']\n","            return_ = preds_aite.map(\n","                lambda x: self.bet(\n","                    race_id, 'wide', [preds_jiku['馬番'].values[0], x], 1\n","                )\n","            ).sum()\n","            n_bets += len(preds_aite)\n","            return_list.append(return_)\n","        elif len(preds_jiku) >= 2:\n","            for umaban in combinations(preds_jiku['馬番'], 2):\n","                return_ += self.bet(race_id, 'wide', umaban, 1)\n","                n_bets += 1\n","            return_list.append(return_)\n","    std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n","    n_hits = np.sum([x>0 for x in return_list])\n","    return_rate = np.sum(return_list) / n_bets\n","    return n_bets, return_rate, n_hits, std\n","def sanrentan_nagashi(self, X, threshold = 1.5, n_aite=7):\n","    pred_table = self.pred_table(X, threshold, bet_only = False)\n","    n_bets = 0\n","    return_list = []\n","    for race_id, preds in pred_table.groupby(level=0):\n","        preds_jiku = preds.query('pred == 1')\n","        if len(preds_jiku) == 1:\n","            continue\n","        elif len(preds_jiku) == 2:\n","            preds_aite = preds.sort_values('score', ascending = False).iloc[2:(n_aite+2)]['馬番']\n","            return_ = preds_aite.map(\n","                lambda x: self.bet(\n","                    race_id, 'sanrentan',\n","                    np.append(preds_jiku['馬番'].values, x),\n","                    1\n","                )\n","            ).sum()\n","            n_bets += len(preds_aite)\n","            return_list.append(return_)\n","        elif len(preds_jiku) >= 3:\n","            return_ = 0\n","            for umaban in permutations(preds_jiku['馬番'], 3):\n","                return_ += self.bet(race_id, 'sanrentan', umaban, 1)\n","                n_bets += 1\n","            return_list.append(return_)\n","    std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n","    n_hits = np.sum([x>0 for x in return_list])\n","    return_rate = np.sum(return_list) / n_bets\n","    return n_bets, return_rate, n_hits, std"]},{"cell_type":"markdown","metadata":{"id":"Mx_r7j2uXq7y"},"source":["# 3: 関数定義"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"TWJArmNfXr9U"},"outputs":[],"source":["# DataFrameをtrain_dataとtest_dataに分割する関数。\n","def split_data(df, test_size=0.3):\n","  sorted_id_list = df.sort_values('date').index.unique()\n","  train_id_list = sorted_id_list[:round(len(sorted_id_list)*(1-test_size))]\n","  test_id_list = sorted_id_list[round(len(sorted_id_list)*(1-test_size)):]\n","  train_data = df.loc[train_id_list]# .drop(['date'], axis=1)\n","  test_data = df.loc[test_id_list]# .drop(['date'], axis=1)\n","  return train_data, test_data\n","def process_categorical(df, target_columns):\n","  df2 = df.copy()\n","  for column in target_columns:\n","    df2[column] = LabelEncoder().fit_transform(df2[column].fillna('Na'))\n","  # target_columns以外にカテゴリ変数があれば、ダミー変数化する\n","  df2 = pd.get_dummies(df2)\n","  for column in target_columns:\n","    df2[column] = df2[column].astype('category')\n","  return df2\n","def gain(return_func, X, n_samples=100, t_range=[0.5, 3.5]):\n","  gain = {}\n","  for i in tqdm(range(n_samples)):\n","    # min_thresholdから1まで、n_samples等分して、thresholdをfor分で回す\n","    threshold = t_range[1] * i / n_samples + t_range[0] * (1-(i/n_samples))\n","    n_bets, return_rate, n_hits, std = return_func(X, threshold)\n","    if n_bets > 2:\n","      gain[threshold] = {'return_rate': return_rate,\n","                          'n_hits': n_hits,\n","                          'std': std,\n","                          'n_bets': n_bets}\n","  return pd.DataFrame(gain).T\n","def update_data(old, new):\n","  filtered_old = old[~old.index.isin(new.index)]\n","  return pd.concat([filtered_old, new])\n","def plot(df, label=' '):\n","  # 標準偏差で幅をつけて薄くプロット\n","  plt.fill_between(df.index,\n","                  y1=df['return_rate']-df['std'],\n","                  y2=df['return_rate']+df['std'],\n","                  alpha=0.3) #alphaで透明度を設定\n","  # plt.fill_between(df['n_bets'],\n","  #                 y1=df['return_rate']-df['std'],\n","  #                 y2=df['return_rate']+df['std'],\n","  #                 alpha=0.3)\n","  #回収率を実線でプロット\n","  plt.plot(df.index, df['return_rate'], label=label)\n","  # plt.plot(df['n_bets'], df['return_rate'], label=label)\n","  plt.legend() #labelで設定した凡例を表示させる\n","  plt.grid(True) #グリッドをつける"]},{"cell_type":"markdown","metadata":{"id":"8GLBEQb0YkWX"},"source":["# 4: データの作成"]},{"cell_type":"markdown","metadata":{"id":"miDnRBfkZJjH"},"source":["## 4.1: rr.data_c"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"NUjmNzLjYsMD"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 524/524 [00:40<00:00, 13.06it/s]\n","100%|██████████| 524/524 [00:39<00:00, 13.28it/s]\n","100%|██████████| 524/524 [00:40<00:00, 12.85it/s]\n"]}],"source":["rr = RaceResults(race_results)\n","\n","# 前処理\n","rr.preprocessing()\n","\n","# 馬の過去成績の追加\n","hr = HorseResults(horse_results)\n","rr.merge_horse_results(hr)\n","\n","# 5世代分の血統データの追加\n","p = Peds(ped_results)\n","p.encode()\n","rr.merge_ped_results(p.ped_results_e)\n","\n","# カテゴリ変数の処理\n","rr.process_categorical()"]},{"cell_type":"markdown","metadata":{"id":"2duyvZVHZMwO"},"source":["## 4.2: st.data_c"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hc9ojuJtZTbQ"},"outputs":[],"source":["# 出馬表データのスクレイピング\n","# 欲しい出馬表のrace_id, 日付を引数とする。\n","# 2021050405: 2021 10/23 東京\n","# url = \"https://db.netkeiba.com/race/\" + race_id\n","\n","race_id_list = ['2021050406{}'.format(str(i).zfill(2)) for i in range(1, 13)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1UwYFMFZT_h"},"outputs":[],"source":["st = ShutubaTable.scrape(race_id_list=race_id_list, date='2021/10/30')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kG_TBGqfZXo6"},"outputs":[],"source":["# 出馬表データを作成する。\n","\n","st.preprocessing()\n","\n","st.merge_horse_results(hr)\n","\n","st.merge_ped_results(p.ped_results_e)\n","\n","st.process_categorical(results_m = rr.data_pe)"]},{"cell_type":"markdown","metadata":{"id":"kzGGEbvFYQ3Z"},"source":["# 6: 賭け用ソフトウェア"]},{"cell_type":"markdown","metadata":{"id":"ByhBw-voYUU0"},"source":["##6.1: optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xT4gqn6IXJwc"},"outputs":[],"source":["X = rr.data_c.drop(['rank', 'date', '単勝', '体重', '体重変化'], axis=1)\n","y = rr.data_c['rank']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ro1F9cEZYaJW"},"outputs":[],"source":["train, valid = split_data(rr.data_c)\n","\n","X_train = train.drop(['rank', 'date', '単勝', '体重', '体重変化'], axis=1)\n","y_train = train['rank']\n","X_valid = valid.drop(['rank', 'date', '単勝', '体重', '体重変化'], axis=1)\n","y_valid = valid['rank']"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"M3RnutVhZivX"},"outputs":[{"ename":"NameError","evalue":"name 'X_train' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-c538f5a3e8b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgb_o\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlgb_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlgb_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"]}],"source":["import optuna.integration.lightgbm as lgb_o\n","\n","lgb_train = lgb_o.Dataset(X_train.values, y_train.values)\n","lgb_valid = lgb_o.Dataset(X_valid.values, y_valid.values)\n","\n","# binary: 予測が0 or 1の時に使う。\n","params = {\n","    'objective': 'binary',\n","    'random_state': 100\n","}\n","\n","lgb_clf_o = lgb_o.train(params,\n","                        lgb_train,\n","                        valid_sets=(lgb_train, lgb_valid),\n","                        verbose_eval=100,\n","                        early_stopping_rounds=10\n","                        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ruy9CWEtZjXS"},"outputs":[],"source":["lgb_clf_o.params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qbYKC_4mZllz"},"outputs":[],"source":["lgb_clf = lgb.LGBMClassifier(**lgb_clf_o.params)\n","lgb_clf.fit(X.values, y.values)"]},{"cell_type":"markdown","metadata":{"id":"U9Bg3VHaZ_hM"},"source":["## 6.2: 複勝馬の予想"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUE6v94yaBOE"},"outputs":[],"source":["# ModelEvaluator\n","me = ModelEvaluator(lgb_clf, ['drive/My Drive/Horse_racing/pickle/overall/return_tables.pickle'])\n","# me = ModelEvaluator(lgb_clf, ['pickle/overall/return_tables.pickle'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-LoVyx5dcJi"},"outputs":[],"source":["X_fact = st.data_c.drop(['date', '体重', '体重変化'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcWalSAudcoC"},"outputs":[],"source":["# MLが予想する信憑性が高い馬n選\n","# pred = me.predict_proba(st.data_c.drop(['date'], axis=1), train=False)\n","pred = me.predict_proba(X_fact, train=False)\n","proba_table = st.data_c[['馬番']].copy()\n","proba_table['score'] = pred\n","proba_table.sort_values('score', ascending = False).head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nrg9aMBGdfjg"},"outputs":[],"source":["# あるscore以上の馬番のみを出力する。\n","# 三連複, 三連単: 1.5前後\n","# 馬連, 馬単: 2.3前後\n","# 単勝: 3.4前後\n","pred_table = me.pred_table(X_fact, threshold=1.3, tansho=False, train=False)\n","for race_id, preds in pred_table.groupby(level=0):\n","  if len(preds) >= 3: # 馬単, 馬連や三連単, 三連複には不必要なデータを取り除くことが可能\n","    display(preds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yUWim6ldhgI"},"outputs":[],"source":["# 予想に用いられている特徴量\n","me.feature_importance(X_fact)"]},{"cell_type":"markdown","metadata":{"id":"zzlkEl70ds0Q"},"source":["## 6.3: 払戻表\n","※ 既に終了しているレースのみ有効"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvo2fRX8dw50"},"outputs":[],"source":["return_tables_today = Return.scrape(race_id_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TjV8--BxeWSW"},"outputs":[],"source":["rt = Return(return_tables_today)"]},{"cell_type":"markdown","metadata":{"id":"8fozXU9TeYb4"},"source":["## 6.4: 当選結果\n","※ 既に終了しているレースのみ有効"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-SuKQZ1ecxY"},"outputs":[],"source":["# レースデータを抽出\n","\n","race_results2 = RaceResults.scrape(race_id_list)\n","rr2 = RaceResults(race_results2)\n","\n","# 前処理\n","rr2.preprocessing()\n","\n","# 馬の過去成績の追加\n","horse_id_list2 = race_results2['horse_id'].unique()\n","hr2 = HorseResults(HorseResults.scrape(horse_id_list2))\n","rr2.merge_horse_results(hr)\n","\n","# 5世代分の血統データの追加\n","p2 = Peds(Peds.scrape(horse_id_list2))\n","p2.encode()\n","rr2.merge_ped_results(p.ped_results_e)\n","\n","# カテゴリ変数の処理\n","rr2.process_categorical()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dcCcg_FTepE2"},"outputs":[],"source":["# 計算の都合に合わせたデータの加工\n","X_results = rr2.data_c.drop(['rank', 'date'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ET6xCDsZeyFH"},"outputs":[],"source":["# 三連複の結果の計算\n","temp = gain(me.sanrenpuku_box, X_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1j5pON6e6F2"},"outputs":[],"source":["# thresholdごとの回収率\n","plot(temp, 'today')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ug1B85eNe-lw"},"outputs":[],"source":["# n_bets, return_rate, n_hits, std\n","me.sanrenpuku_box(X_fact, threshold=1.2, show=True)"]},{"cell_type":"markdown","metadata":{"id":"LuRjoKfyfGtd"},"source":["#7: 交差検証ソフトウェア"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAXs_ZTefzie"},"outputs":[],"source":["# 訓練データ、検証データ、テストデータに分ける\n","train, test = split_data(rr.data_c)\n","train, valid = split_data(train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTNNoUmMf2K_"},"outputs":[],"source":["X_train = train.drop(['rank', 'date', '単勝'], axis=1)\n","y_train = train['rank']\n","X_valid = valid.drop(['rank', 'date', '単勝'], axis=1)\n","y_valid = valid['rank']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJQj0ovrf2fM"},"outputs":[],"source":["import optuna.integration.lightgbm as lgb_o\n","\n","lgb_train = lgb_o.Dataset(X_train.values, y_train.values)\n","lgb_valid = lgb_o.Dataset(X_valid.values, y_valid.values)\n","\n","# binary: 予測が0 or 1の時に使う。\n","params = {\n","    'objective': 'binary',\n","    'random_state': 100\n","}\n","\n","lgb_clf_o = lgb_o.train(params,\n","                        lgb_train,\n","                        valid_sets=(lgb_train, lgb_valid),\n","                        verbose_eval=100,\n","                        early_stopping_rounds=10\n","                        )"]},{"cell_type":"markdown","metadata":{"id":"uKEA8pnOf9-q"},"source":["  * feature_pre_filter: (LightGBM 3.0からできたもの。)min_data_in_leaf=min_child_samplesをチューニングする時にはFalseにする。\n","  * lambda_l1, lambda_l2: 正則化。過学習を防止する。\n","  * num_leaves: 葉の数\n","  * feature_fraction: 特徴量の60%だけを選んで一つの木を育てる。\n","  * bagging_fraction: データを100%使って一つの木を育てる。\n","  * bagging_freq: バギング(データの水増し)する頻度。今回はバギングしない。\n","  * min_child_samples: 最終的に一つの葉に残るデータ数。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aiAVjmqef6DM"},"outputs":[],"source":["lgb_clf_o.params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQgj1pLkgAmT"},"outputs":[],"source":["# 時系列に沿って訓練データとテストデータに分ける\n","train, test = split_data(rr.data_c)\n","X_train = train.drop(['rank', 'date', '単勝'], axis=1)\n","y_train = train['rank']\n","X_test = test.drop(['rank', 'date'], axis=1)\n","y_test = test['rank']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LUIbsIxgCFJ"},"outputs":[],"source":["lgb_clf = lgb.LGBMClassifier(**lgb_clf_o.params)\n","lgb_clf.fit(X_train.values, y_train.values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bg42KPDAgHTi"},"outputs":[],"source":["# ModelEvaluator\n","me = ModelEvaluator(lgb_clf, ['drive/My Drive/Horse_racing/pickle/overall/return_tables.pickle'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H-yuomfzgKbw"},"outputs":[],"source":["# 予想に用いられている特徴量\n","me.feature_importance(X_test.drop(['単勝'], axis=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tlw7CybOgXDH"},"outputs":[],"source":["# 過去の交差検証結果\n","# g_tansho=pd.read_pickle('drive/My Drive/Horse_racing/pickle/results/g_tansho.pickle')\n","# g_proper=pd.read_pickle('drive/My Drive/Horse_racing/pickle/results/g_proper.pickle')\n","# g_umaren=pd.read_pickle('drive/My Drive/Horse_racing/pickle/results/g_umaren.pickle')\n","# g_umatan=pd.read_pickle('drive/My Drive/Horse_racing/pickle/results/g_umatan.pickle')\n","# g_wide=pd.read_pickle('drive/My Drive/Horse_racing/pickle/results/g_wide.pickle')\n","# g_sanrentan=pd.read_pickle('drive/My Drive/Horse_racing/pickle/results/g_sanrentan.pickle')\n","# g_sanrenpuku=pd.read_pickle('drive/My Drive/Horse_racing/pickle/results/g_sanrenpuku.pickle')\n","# g_umaren_nagashi=pd.read_pickle('drive/My Drive/Horse_racing/pickle/results/g_umaren_nagashi.pickle')\n","# g_umatan_nagashi=pd.read_pickle('drive/My Drive/Horse_racing/pickle/results/g_umatan_nagashi.pickle')\n","# g_wide_nagashi=pd.read_pickle('drive/My Drive/Horse_racing/pickle/results/g_wide_nagashi.pickle')\n","# g_sanrentan_nagashi=pd.read_pickle('drive/My Drive/Horse_racing/pickle/results/g_sanrentan_nagashi.pickle')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HehABA-Pgh6B"},"outputs":[],"source":["# 回収率の計算\n","# g_fukusho = gain(me.fukusho_return, X_test)\n","# g_proper = gain(me.tansho_return_proper, X_test)\n","# g_tansho = gain(me.tansho_return, X_test)\n","# g_umaren = gain(me.umaren_box, X_test)\n","# g_umatan = gain(me.umatan_box, X_test)\n","# g_wide = gain(me.wide_box, X_test)\n","# g_sanrentan = gain(me.sanrentan_box, X_test)\n","# g_sanrenpuku = gain(me.sanrenpuku_box, X_test)\n","\n","# g_umaren_nagashi = gain(me.umaren_nagashi, X_test)\n","# g_umatan_nagashi = gain(me.umatan_nagashi, X_test)\n","# g_wide_nagashi = gain(me.wide_nagashi, X_test)\n","# g_sanrentan_nagashi = gain(me.sanrentan_nagashi, X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaeaNRMkgvBZ"},"outputs":[],"source":["# 横軸: 閾値\n","# 縦軸: 回収率\n","\n","plt.figure(figsize=(10, 8))\n","plot(g_tansho, 'tansho')\n","plot(g_proper, 'proper')\n","plot(g_umaren, 'umaren')\n","plot(g_umatan, 'umatan')\n","plot(g_wide, 'wide')\n","plot(g_sanrentan, 'sanrentan')\n","plot(g_sanrenpuku, 'sanrenpuku')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AEgHJL9ghShS"},"outputs":[],"source":["# 横軸: 閾値\n","# 縦軸: 回収率\n","\n","plt.figure(figsize=(10, 8))\n","plot(g_umaren_nagashi, 'umaren_nagashi')\n","plot(g_umatan_nagashi, 'umatan_nagashi')\n","plot(g_wide_nagashi, 'wide_nagashi')\n","plot(g_sanrentan_nagashi, 'sanrentan_nagashi')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"20DtRv2lg9pP"},"outputs":[],"source":["# 横軸: 賭け枚数\n","# 縦軸: 回収率\n","# 賭け枚数が少ない→一つの当たりの影響が大きい→賭け枚数が多いところでの期待値も＋になることが理想。\n","\n","plt.figure(figsize=(10, 8))\n","plt.plot(g_proper['n_bets'], g_proper['return_rate'], label='proper')\n","plt.plot(g_tansho['n_bets'], g_tansho['return_rate'], label='tansho')\n","plt.plot(g_umaren['n_bets'], g_umaren['return_rate'], label='umaren')\n","plt.plot(g_umatan['n_bets'], g_umatan['return_rate'], label='umatan')\n","plt.plot(g_wide['n_bets'], g_wide['return_rate'], label='wide')\n","plt.plot(g_sanrentan['n_bets'], g_sanrentan['return_rate'], label='sanrentan')\n","plt.plot(g_sanrenpuku['n_bets'], g_sanrenpuku['return_rate'], label='sarenpuku')\n","plt.hlines(1, xmin=-10, xmax=2000) # y=1の直線\n","plt.xlim(-10, 2000)\n","plt.legend()\n","plt.grid(True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tjmeY4k3hUZH"},"outputs":[],"source":["# 横軸: 賭け枚数\n","# 縦軸: 回収率\n","\n","plt.figure(figsize=(10, 8))\n","plt.plot(g_umaren_nagashi['n_bets'], g_umaren_nagashi['return_rate'], label='umaren_nagashi')\n","plt.plot(g_umatan_nagashi['n_bets'], g_umatan_nagashi['return_rate'], label='umatan_nagashi')\n","plt.plot(g_wide_nagashi['n_bets'], g_wide_nagashi['return_rate'], label='wide_nagashi')\n","plt.plot(g_sanrentan_nagashi['n_bets'], g_sanrentan_nagashi['return_rate'], label='sanrentan_nagashi')\n","plt.hlines(1, xmin=-10, xmax=2000) # y=1の直線\n","plt.xlim(-10, 2000)\n","plt.legend()\n","plt.grid(True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j7Yqma1KhCpi"},"outputs":[],"source":["# シャープレシオ\n","\n","plt.figure(figsize=(10, 8))\n","plt.plot(g_proper['n_bets'], (g_proper['return_rate'] - 1) / g_proper['std'], label='proper')\n","plt.plot(g_tansho['n_bets'], (g_tansho['return_rate'] - 1) / g_tansho['std'], label='tansho')\n","plt.plot(g_umaren['n_bets'], (g_umaren['return_rate'] - 1) / g_umaren['std'], label='umaren')\n","plt.plot(g_umatan['n_bets'], (g_umatan['return_rate'] - 1) / g_umatan['std'], label='umatan')\n","plt.plot(g_wide['n_bets'], (g_wide['return_rate'] - 1) / g_wide['std'], label='wide')\n","plt.plot(g_sanrentan['n_bets'], (g_sanrentan['return_rate'] - 1) / g_sanrentan['std'], label='sanrentan')\n","plt.plot(g_sanrenpuku['n_bets'], (g_sanrenpuku['return_rate'] - 1) / g_sanrenpuku['std'], label='sanrenpuku')\n","plt.xlim(-10, 200)\n","plt.ylim(-10, 5)\n","plt.legend()\n","plt.grid()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hodzoh17hWYu"},"outputs":[],"source":["# シャープレシオ\n","\n","plt.figure(figsize=(10, 8))\n","plt.plot(g_umaren_nagashi['n_bets'], (g_umaren_nagashi['return_rate'] - 1) / g_umaren_nagashi['std'], label='umaren_nagashi')\n","plt.plot(g_umatan_nagashi['n_bets'], (g_umatan_nagashi['return_rate'] - 1) / g_umatan_nagashi['std'], label='umatan_nagashi')\n","plt.plot(g_wide_nagashi['n_bets'], (g_wide_nagashi['return_rate'] - 1) / g_wide_nagashi['std'], label='wide_nagashi')\n","plt.plot(g_sanrentan_nagashi['n_bets'], (g_sanrentan_nagashi['return_rate'] - 1) / g_sanrentan_nagashi['std'], label='sanrentan_nagashi')\n","plt.xlim(-10, 2000)\n","plt.ylim(-10, 5)\n","plt.legend()\n","plt.grid()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMwPnC839ikoEMvoUawPx5o","collapsed_sections":[],"name":"Prediction_App.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
